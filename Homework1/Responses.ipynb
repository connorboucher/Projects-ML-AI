{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"Task 1\nI want to predict the likelihood that given the global number of sales for a videogame, the number of \nsales in the NA region specifically is at least 50% of total sales.  I believe this is a good question \nfor logistic regression to solve as logistic regression is well suited to estimating trends that an \noutcome y will occur given an input x.\n\nTask 2\nDataset Used: https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings\n\nFor the data I used, I looked at only the columns pertaining to global sales and to North American sales.\nI did not use data from columns containing the following information: sales in any other locations locals,\ngame titles, genres, years of release, publishers, developers, age ratings, or scores by critics or users.\nI copied these two columns over to a separate file titled \"Video_Game_Sales_Revised.csv\" which I have\nadded to my Git repo since I was not using the pandas package when readin the file. I plan to install\npandas for future homeworks to be able to read directly from the original file.\n\nWhen going through the data in the columns I am utilizing, I am only using rows that contain non-null values\nfor both global sales and North American sales.  The only rows that did not contain numerical values were\nthe column titles, so I did not have to estimate or remove columns with empty values.\n\nTask 3\nCost Function : the MSE function (1/n) Σ (n/2) * ((x . theta) - y)^2\n    the summation is from 1 to n where n is the number of data entries in my columns\n    also . represents the dot product of x and theta\n    \nDerivatives: (1/n) Σ (theta0 - theta1 * x_i) - y_i\n             ((1/n) Σ (theta0 - theta1 * x_i) - y_i) * x_i\n        both summations are from 1 to n\n        these are the derivatives calculated by using MSE\n\nTask 4\nI used both the Adam and Nadam optimization algorithms as provided by tensorflow's packages. Both performed\nvery similarly, with both having losses falling in the range of 2 to 4.  Nadam tended to\nhave a slightly lower loss on average than Adam, but both had much lower loses than the gradient\ndescents I coded myself.  My stocashtic descent tended to have a cost of around 1000 and my batch gradient\ndescent trended towards having loses of around 21000.  I think optimization algorithms are much better suited\nto this type of data as there appears to not be very much of a direct correlation between my x and y values,\nmeaning a lot of loss is caused when using gradient descent algorithms before they are optimized.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}